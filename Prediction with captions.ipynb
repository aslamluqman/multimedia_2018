{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting video memorability using captions (Jointly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the captions and the memorability scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stone/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from keras import Sequential\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "import pyprind\n",
    "from collections import Counter\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducability\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/media/stone/Data/DataSet_me18me/me18me-devset/dev-set/dev-set_video-captions.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-be0ed756a42f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#cap_path = '/media/win/Users/ecelab-adm/Desktop/DataSet_me18me/me18me-devset/dev-set/dev-set_video-captions.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mcap_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/media/stone/Data/DataSet_me18me/me18me-devset/dev-set/dev-set_video-captions.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mdf_cap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_caps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcap_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# load the ground truth values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-be0ed756a42f>\u001b[0m in \u001b[0;36mread_caps\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mpairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/media/stone/Data/DataSet_me18me/me18me-devset/dev-set/dev-set_video-captions.txt'"
     ]
    }
   ],
   "source": [
    "# load labels and captions\n",
    "def read_caps(fname):\n",
    "    \"\"\"Load the captions into a dataframe\"\"\"\n",
    "    vn = []\n",
    "    cap = []\n",
    "    df = pd.DataFrame();\n",
    "    with open(fname) as f:\n",
    "        for line in f:\n",
    "            pairs = line.split()\n",
    "            vn.append(pairs[0])\n",
    "            cap.append(pairs[1])\n",
    "        df['video']=vn\n",
    "        df['caption']=cap\n",
    "    return df\n",
    "\n",
    "\n",
    "# load the captions\n",
    "#cap_path = '/media/win/Users/ecelab-adm/Desktop/DataSet_me18me/me18me-devset/dev-set/dev-set_video-captions.txt'\n",
    "cap_path = '/media/stone/Data/DataSet_me18me/me18me-devset/dev-set/dev-set_video-captions.txt'\n",
    "df_cap=read_caps(cap_path)\n",
    "\n",
    "# load the ground truth values\n",
    "label_path = '/media/stone/Data/DataSet_me18me/me18me-devset/dev-set/ground-truth/'\n",
    "labels=pd.read_csv(label_path+'ground-truth_dev-set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = Counter()\n",
    "# setup prograss tracker\n",
    "pbar = pyprind.ProgBar(len(df_cap['caption']), title='Counting word occurrences')\n",
    "for i, cap in enumerate(df_cap['caption']):\n",
    "    # replace punctuations with space\n",
    "    # convert words to lower case \n",
    "    text = ''.join([c if c not in punctuation else ' ' for c in cap]).lower()\n",
    "    df_cap.loc[i,'caption'] = text\n",
    "    pbar.update()\n",
    "    counts.update(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cap.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing the captions for model inputs\n",
    "### Separate words and count each word's occurrence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maping each unique word to an integer (one-hot encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the word index\n",
    "len_token = len(counts)\n",
    "tokenizer = Tokenizer(num_words=len_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(list(df_cap.caption.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_res = tokenizer.texts_to_matrix(list(df_cap.caption.values),mode='binary')\n",
    "sequences = tokenizer.texts_to_sequences(list(df_cap.caption.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences[:2]\n",
    "len(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making them the same length by padding zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating max length\n",
    "max_len = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making sequences index same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_seq = np.zeros((len(sequences),max_len))\n",
    "for i in range(len(sequences)):\n",
    "    n = len(sequences[i])\n",
    "    if n==0:\n",
    "        print(i)\n",
    "    else:\n",
    "        X_seq[i,-n:] = sequences[i]\n",
    "X_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making one-hot encoding the same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is already the same size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Building network model\n",
    "## Split data set to training and test data\n",
    "## Prediction with indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = labels[['short-term_memorability','long-term_memorability']].values\n",
    "X = X_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add dropout\n",
    "# add regularizers\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(10,activation='relu',kernel_regularizer=regularizers.l2(0.0005),input_shape=(max_len,)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(10,activation='relu',kernel_regularizer=regularizers.l2(0.0005)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(2,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model \n",
    "model.compile(optimizer='rmsprop',loss='mse',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model \n",
    "history = model.fit(X_train,Y_train,epochs=20,validation_data=(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the validation and training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1,len(loss)+1)\n",
    "\n",
    "plt.plot(epochs,loss,'bo',label='Training loss')\n",
    "plt.plot(epochs,val_loss,'b',label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Acc')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting video memorability using captions (one-hot res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = labels[['short-term_memorability','long-term_memorability']].values\n",
    "X = one_hot_res;\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# add dropout\n",
    "# add regularizers\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(10,activation='relu',kernel_regularizer=regularizers.l2(0.001),input_shape=(len_token,)))\n",
    "model.add(layers.Dropout(0.6))\n",
    "model.add(layers.Dense(10,activation='relu',kernel_regularizer=regularizers.l2(0.001)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(2,activation='sigmoid'))\n",
    "\n",
    "# compile the model \n",
    "model.compile(optimizer='rmsprop',loss='mse',metrics=['accuracy'])\n",
    "\n",
    "# training the model \n",
    "history = model.fit(X_train,Y_train,epochs=20,validation_data=(X_test,Y_test))\n",
    "\n",
    "# visualizing the model\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1,len(loss)+1)\n",
    "\n",
    "plt.plot(epochs,loss,'bo',label='Training loss')\n",
    "plt.plot(epochs,val_loss,'b',label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Acc')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
